{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2c3bb37a290>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "from torch.nn import functional as F \n",
    "torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Preparing training data\n",
    "2. Definition of a first simple model, ony using embeddings\n",
    "3. Building a transformer decoder\n",
    "4. Final notes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preparing training data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/input.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1115394"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unique characters present in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(65, \"\\n !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "characters = sorted(list(set(text)))\n",
    "vocab_size = len(characters)\n",
    "vocab_size, \"\".join(characters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Index to character and character to index functions for encoding and decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([46, 43, 50, 50, 53], 'hello')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stoi = {ch:i for i,ch in enumerate(characters)}\n",
    "itos = {i:ch for i,ch in enumerate(characters)}\n",
    "\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: \"\".join([itos[i] for i in l])\n",
    "\n",
    "encode(\"hello\"), decode(encode(\"hello\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Representation of the original as a sequence of integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.tensor(encode(text),dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "test_data = data[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only Chunks of the dataset are fed to the model. They have a max length called block size, also called context length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8\n",
    "train_data[:block_size+1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " As the model is always predicting the next word, x,y sets can be created by sampling chunks from the dataset which are offset by one character. The transformer can be trained on all the examples below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input tensor([18]), target 47\n",
      "input tensor([18, 47]), target 56\n",
      "input tensor([18, 47, 56]), target 57\n",
      "input tensor([18, 47, 56, 57]), target 58\n",
      "input tensor([18, 47, 56, 57, 58]), target 1\n",
      "input tensor([18, 47, 56, 57, 58,  1]), target 15\n",
      "input tensor([18, 47, 56, 57, 58,  1, 15]), target 47\n",
      "input tensor([18, 47, 56, 57, 58,  1, 15, 47]), target 58\n"
     ]
    }
   ],
   "source": [
    "x= train_data[:block_size]\n",
    "y= train_data[1:block_size+1]\n",
    "\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"input {context}, target {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A helper function to sample random datapoints, of a given length from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split):\n",
    "    data= train_data if split == \"train\" else test_data\n",
    "    ix = torch.randint(len(data) -block_size, (batch_size,)) # Random sampling batch_size indexes from the dataset to use as starting points for chunks\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix]) # Stacking the results up in a batch_size x chunk_size tensor\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix]) # Offset by one because they are the targets of x\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[43, 44, 43, 56,  1, 51, 43,  1],\n",
      "        [39, 57, 39, 52, 58, 10,  1, 57],\n",
      "        [43, 56,  7, 40, 63,  1, 58, 53],\n",
      "        [44, 53, 56,  6,  1, 43, 56, 43]])\n",
      "targets:\n",
      "torch.Size([4, 8])\n",
      "tensor([[44, 43, 56,  1, 51, 43,  1, 58],\n",
      "        [57, 39, 52, 58, 10,  1, 57, 39],\n",
      "        [56,  7, 40, 63,  1, 58, 53,  1],\n",
      "        [53, 56,  6,  1, 43, 56, 43,  1]])\n"
     ]
    }
   ],
   "source": [
    "xb, yb = get_batch(\"train\")\n",
    "print(\"inputs:\")\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print(\"targets:\")\n",
    "print(yb.shape)\n",
    "print(yb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Representation of the individual training samples of a block and their respective targets. Each training example can be used block_size times for training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: [43], target: 44\n",
      "Context: [43, 44], target: 43\n",
      "Context: [43, 44, 43], target: 56\n",
      "Context: [43, 44, 43, 56], target: 1\n",
      "Context: [43, 44, 43, 56, 1], target: 51\n",
      "Context: [43, 44, 43, 56, 1, 51], target: 43\n",
      "Context: [43, 44, 43, 56, 1, 51, 43], target: 1\n",
      "Context: [43, 44, 43, 56, 1, 51, 43, 1], target: 58\n",
      "Context: [39], target: 57\n",
      "Context: [39, 57], target: 39\n",
      "Context: [39, 57, 39], target: 52\n",
      "Context: [39, 57, 39, 52], target: 58\n",
      "Context: [39, 57, 39, 52, 58], target: 10\n",
      "Context: [39, 57, 39, 52, 58, 10], target: 1\n",
      "Context: [39, 57, 39, 52, 58, 10, 1], target: 57\n",
      "Context: [39, 57, 39, 52, 58, 10, 1, 57], target: 39\n",
      "Context: [43], target: 56\n",
      "Context: [43, 56], target: 7\n",
      "Context: [43, 56, 7], target: 40\n",
      "Context: [43, 56, 7, 40], target: 63\n",
      "Context: [43, 56, 7, 40, 63], target: 1\n",
      "Context: [43, 56, 7, 40, 63, 1], target: 58\n",
      "Context: [43, 56, 7, 40, 63, 1, 58], target: 53\n",
      "Context: [43, 56, 7, 40, 63, 1, 58, 53], target: 1\n",
      "Context: [44], target: 53\n",
      "Context: [44, 53], target: 56\n",
      "Context: [44, 53, 56], target: 6\n",
      "Context: [44, 53, 56, 6], target: 1\n",
      "Context: [44, 53, 56, 6, 1], target: 43\n",
      "Context: [44, 53, 56, 6, 1, 43], target: 56\n",
      "Context: [44, 53, 56, 6, 1, 43, 56], target: 43\n",
      "Context: [44, 53, 56, 6, 1, 43, 56, 43], target: 1\n"
     ]
    }
   ],
   "source": [
    "for b in range(batch_size):\n",
    "    for t in range(block_size):\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b,t]\n",
    "        print(f\"Context: {context.tolist()}, target: {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Definition of a first simple model, ony using embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple bigram model, only using embeddings as paramters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the embeddings, the indexes are used. Each index has its own embedding, so when i.e. the 24th index is queried, the 24th row from the embedding table is returned. In the model below, the next token is only predicted based on a single token. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "    \n",
    "    def forward(self, idx, targets=None):\n",
    "        logits = self.token_embedding_table(idx) # Dimensions: Batch, Time, Channel\n",
    "\n",
    "        if targets is None:\n",
    "            loss=None\n",
    "        else:\n",
    "            # Torch expects a different dimension for crossentropy\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, loss = self(idx)\n",
    "\n",
    "            # Only the last time step will be considered \n",
    "            logits = logits[:, -1, :]\n",
    "\n",
    "            # Get probablities\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "            # sampling from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "            #appeding the results\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "        return idx    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some output of an untrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n",
      "\n",
      "l-QYjt'CL?jLDuQcLzy'RIo;'KdhpV\n",
      "vLixa,nswYZwLEPS'ptIZqOZJ$CA$zy-QTkeMk x.gQSFCLg!iW3fO!3DGXAqTsq3pdgq\n"
     ]
    }
   ],
   "source": [
    "m = BigramModel(vocab_size=vocab_size)\n",
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape)\n",
    "loss\n",
    "\n",
    "idx = torch.zeros((1,1), dtype=torch.long)\n",
    "print(decode(m.generate(idx=idx, max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5589075088500977\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "for steps in range(10000):\n",
    "    xb, yb = get_batch(\"train\")\n",
    "\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output of a trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iyoteng h hasbe pave pirance\n",
      "Rie hicomyonthar's\n",
      "Plinseard ith henoure wounonthioneir thondy, y heltieiengerofo'dsssit ey\n",
      "KIN d pe wither vouprrouthercc.\n",
      "hathe; d!\n",
      "My hind tt hinig t ouchos tes; st yo hind wotte grotonear 'so it t jod weancotha:\n",
      "h hay.JUCle n prids, r loncave w hollular s O:\n",
      "HIs; ht anjx?\n",
      "\n",
      "DUThinqunt.\n",
      "\n",
      "LaZAnde.\n",
      "athave l.\n",
      "KEONH:\n",
      "ARThanco be y,-hedarwnoddy scace, tridesar, wnl'shenou\n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(idx=idx, max_new_tokens=400)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Building a tranformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following we build on the model from before and add the elements of a transformer decoder to it.\n",
    "The visualization of the decoder architecture from the attention is all you need paper:\n",
    "\n",
    "![](images/decoder.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Positional embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Self-attention mechanisms, while powerful for modelling relationships between tokens in a sequence, lack an inherent understanding of the order of those tokens. This is because attention operates over a set of vectors without any inherent notion of their spatial arrangement. To address this limitation, positional embeddings are introduced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example of how positional embeddings could be implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embed) # embedding for each position\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        token_embeddings = self.token_embedding_table(idx) # Dimensions: Batch, Time, vocab size\n",
    "        positional_embedding = self.position_embedding_table(torch.arange(T)) # integers until T-1, Embeeded to create a T, C matrix\n",
    "        x = token_embeddings + positional_embedding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Self attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Self attention can be seen as a communication mechanism within a network of nodes, analogous to a directed graph. Each node in this network possesses a vector of information.\n",
    "\n",
    "Affinities between nodes are determined by calculating dot products between the query vector of one node and the key vectors of all other nodes. Higher dot products indicate stronger affinities, suggesting the nodes find each other's information relevant.\n",
    "\n",
    "The fundamental goal of self-attention is to allow these nodes to exchange information in a data-dependent manner, meaning the flow of information is determined by the content of the data itself. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Masking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A triangular masking mechanism is used to prevent future tokens (words) from influencing past tokens. This ensures a unidirectional flow of information, essential for predicting upcoming words in a sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "● B: Batch size - the number of independent sequences being processed simultaneously. <br>\n",
    "● T: Time - the maximum context length for making predictions.<br>\n",
    "● C: Channels - in this context, refers to the embedding dimensions.<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B, T, C = 4,8,2\n",
    "x = torch.randn(B, T, C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the average value of the context of each token. They are a lossy compression of the contextual relevance and only used as an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.4241, 2.3706],\n",
       "         [0.2898, 1.2700],\n",
       "         [0.1273, 1.0406],\n",
       "         [0.1800, 0.6874],\n",
       "         [0.3613, 0.5511],\n",
       "         [0.2946, 0.1472],\n",
       "         [0.2726, 0.4710],\n",
       "         [0.0927, 0.5778]]),\n",
       " tensor([[ 0.4241,  2.3706],\n",
       "         [ 0.1555,  0.1694],\n",
       "         [-0.1978,  0.5817],\n",
       "         [ 0.3381, -0.3723],\n",
       "         [ 1.0866,  0.0062],\n",
       "         [-0.0391, -1.8723],\n",
       "         [ 0.1407,  2.4141],\n",
       "         [-1.1666,  1.3247]]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow = torch.zeros((B,T,C)) #bow = bag of words, init at 0\n",
    "for b in range(B): # iteration over batch\n",
    "    for t in range(T): # terating over time\n",
    "        xprev = x[b, :t+1] # context including current token itself \n",
    "        xbow[b,t] = torch.mean(xprev, 0) \n",
    "xbow[0], x[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.tril creates an lower triangular matrix, perfect for vectorization and filtering out future tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei = torch.tril(torch.ones(T,T))\n",
    "wei"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using matmul to make this process more efficient. By normalizing the values of torch.tril(), the averaging process can be vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "wei = wei / wei.sum(1, keepdim = True)\n",
    "xbow2 = wei @ x # (T, T) @ (B, T, C) -> torch creates batch dimension -> (B, T, T) @ (B, T, C) ----> (B, T, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.4241, 2.3706],\n",
       "         [0.2898, 1.2700],\n",
       "         [0.1273, 1.0406],\n",
       "         [0.1800, 0.6874],\n",
       "         [0.3613, 0.5511],\n",
       "         [0.2946, 0.1472],\n",
       "         [0.2726, 0.4710],\n",
       "         [0.0927, 0.5778]]),\n",
       " tensor([[0.4241, 2.3706],\n",
       "         [0.2898, 1.2700],\n",
       "         [0.1273, 1.0406],\n",
       "         [0.1800, 0.6874],\n",
       "         [0.3613, 0.5511],\n",
       "         [0.2946, 0.1472],\n",
       "         [0.2726, 0.4710],\n",
       "         [0.0927, 0.5778]]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow2[0], xbow[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way of achieving the same result using softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tril = torch.tril(torch.ones(T,T))\n",
    "tril"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei = torch.zeros((T, T))\n",
    "wei = wei.masked_fill(tril==0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=1) # Softmax is a normalization operation, thats why the result is the same. \n",
    "xbow3 = wei @ x\n",
    "torch.allclose(xbow, xbow3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.4241,  2.3706],\n",
       "         [ 0.2898,  1.2700],\n",
       "         [ 0.1273,  1.0406],\n",
       "         [ 0.1800,  0.6874],\n",
       "         [ 0.3613,  0.5511],\n",
       "         [ 0.2946,  0.1472],\n",
       "         [ 0.2726,  0.4710],\n",
       "         [ 0.0927,  0.5778]],\n",
       "\n",
       "        [[ 0.7175,  0.1091],\n",
       "         [ 0.6630,  0.0273],\n",
       "         [ 0.5679,  0.2643],\n",
       "         [ 0.0765,  0.2857],\n",
       "         [-0.0240,  0.3696],\n",
       "         [ 0.1839,  0.0726],\n",
       "         [-0.0977, -0.0122],\n",
       "         [-0.0985,  0.0877]],\n",
       "\n",
       "        [[ 0.6477,  0.4192],\n",
       "         [ 0.5112,  0.4835],\n",
       "         [ 0.3243,  0.2685],\n",
       "         [ 0.0693,  0.2880],\n",
       "         [ 0.2925,  0.1148],\n",
       "         [ 0.1442,  0.1071],\n",
       "         [ 0.0084,  0.4233],\n",
       "         [ 0.1843,  0.5972]],\n",
       "\n",
       "        [[ 0.6527,  0.0654],\n",
       "         [ 0.1873, -0.7585],\n",
       "         [-0.4758, -0.6699],\n",
       "         [-0.3655, -0.6324],\n",
       "         [-0.2404, -0.6654],\n",
       "         [-0.4016, -0.6217],\n",
       "         [-0.6164, -0.6241],\n",
       "         [-0.4988, -0.5355]]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self attention implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation of actual self attention, instead of averages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for every single token/ node a query and a key vector is created. <br>\n",
    "Query = what am I looking for <br>\n",
    "Key = What do i contain <br>\n",
    "Affinities/ relevancy between tokens = dot product of key and query <br>\n",
    "The dot product of one query and all other keys will be calculated(wei in the case below). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B, T, C = 4,8,32 \n",
    "x = torch.randn(B, T, C) # Matrix of random embeddings\n",
    "\n",
    "#single head of attention\n",
    "head_size=16\n",
    "\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "\n",
    "k = key(x) # projection into B, T, head_size\n",
    "q = query(x) # projection into  B, T, head_size\n",
    "\n",
    "#Calculation of affinities through dot products\n",
    "wei = q @ k.transpose(-2, -1) # Batch dimensions shouldnt be transposed. Produces # B, T, 16 @ B, 16, T ---> B, T, T\n",
    "\n",
    "tril = torch.tril(torch.ones(T,T)) # mask\n",
    "wei = wei.masked_fill(tril==0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=1) # Softmax is a normalization operation, thats why the result is the same. \n",
    "\n",
    "v = value(x) # projection into  B, T, head_size\n",
    "out = wei @ v \n",
    "\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In self attention the batches act independently of each other. Attention \"communication\" only happens inside a sequence\n",
    "- Right now there is no notion of space\n",
    "- This whole process can be seen as a directed graph where the weights describe the affinity\n",
    "- The above example would be a \"decoder\" attention block which would be used in an autoregressive manner, therefore the masking.\n",
    "- An encoder would not use this mask as use cases such as sentiment analysis, which dont work autoregressive, do not require that condition.\n",
    "- This example is called self attention because they all come from the same source. In the tranformer model, cross attention uses several sources of information\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, q @ k needs to be scaled by the square root of the headsize for normalization. this has the following reasons:\n",
    "● Variance Control: When the query (Q) and key (K) vectors are initialised with random values, the dot product Q⋅K can result in values with high variance. This high variance can lead to unstable gradients during training, making it challenging for the model to learn effectively.\n",
    "\n",
    "● Softmax Sensitivity: Softmax function, used to normalise attention scores into a probability distribution, is very sensitive to large input values. If the dot products are too large, the softmax function can create extremely peaky distributions, essentially focusing attention on a single token and ignoring others.\n",
    "\n",
    "● Stabilising Attention Weights: Dividing by the square root of the head size helps control the variance of the dot products, ensuring a more stable distribution of attention weights. This prevents the softmax function from creating overly sharp peaks and allows for a more balanced flow of information between tokens. Without scaling, softmax results| will converge to one hot vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example of the variances being very different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = torch.randn(B,T, head_size)\n",
    "q = torch.randn(B,T, head_size)\n",
    "\n",
    "#Calculation of affinities through dot products\n",
    "wei = q @ k.transpose(-2, -1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0449)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0700)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(17.4690)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei.var()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling wei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0682)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei = wei * head_size ** -1\n",
    "wei.var()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-head attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to feature maps in CNNs, transformers use multiple heads of attention, each potentially focussing on different aspects of a sequence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_embed=32\n",
    "\n",
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.head_size = head_size\n",
    "        self.key = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embed, head_size, bias=False)\n",
    "        \n",
    "        self.register_buffer(\"tril\", torch.tril(torch.ones(block_size, block_size)))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        B, T, C = x.shape\n",
    "\n",
    "        k = self.key(x) # projection into B, T, head_size\n",
    "        q = self.query(x) # projection into  B, T, head_size\n",
    "        v = self.value(x)\n",
    "\n",
    "        self_attention = q @ k.transpose(-2, -1) * C**-0.5 # affinities and scaling\n",
    "\n",
    "        self_attention = self_attention.masked_fill(self.tril[:T, :T]==0, float('-inf')) # applying the mask\n",
    "        self_attention = F.softmax(self_attention, dim=-1) # Softmax is a normalization operation, thats why the result is the same. \n",
    "\n",
    "        self_attention = self_attention @ v\n",
    "\n",
    "        return self_attention\n",
    "\n",
    "class MultiheadAttention(nn.Module):\n",
    "    def __init__(self,  num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([AttentionHead(head_size=head_size) for _ in range(num_heads)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        multi_head_attention = torch.cat([h(x) for h in self.heads], dim=-1) # cat over channel dim\n",
    "        return multi_head_attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually, the length of the embeddings is divided by the number of heads during the input, to get an n_embed sized output. <br>\n",
    "\n",
    "e.g n_heads = 4, n_embed = 32. <br>\n",
    "each attention head outputs 32 / 4  = 8 sized results <br>\n",
    "concat. brings back a 32 sized result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self_attention_heads = MultiheadAttention(4, n_embed//4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Feed forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the tokens have communicated with each other during the self-attention stage, they need to \"think\" on the data they received. The feedforward layer allows this individual processing to occur.<br>\n",
    "The feedforward layer is applied independently to all tokens. <br>\n",
    "In the original Transformer paper, \"Attention is All You Need\", the feedforward network is referred to as a \"position-wise feed-forward network\", which is simply a multi-layer perceptron (MLP)\n",
    "\n",
    "Crucially, the feedforward network operates on each token's representation independently. This allows individual tokens to process the gathered contextual information and refine their representations based on their specific role and meaning within the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embed) -> None:\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(nn.Linear(n_embed, n_embed),\n",
    "                                nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Putting everything together into a single transformer block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The attention mechanism and the feedforward neural net will be combined into a single transformer block. In the original paper, this block is repeated many times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, n_embed, n_head):\n",
    "        super.__init__()\n",
    "        head_size = n_embed // n_head\n",
    "        self.self_attention = MultiheadAttention(n_head, head_size)\n",
    "        self.ff = FeedForward(n_embed=n_embed)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.self_attention(x)\n",
    "        x = x + self.ff(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model below already implements a very deep neural network. These tend to suffer from optimization issues such as exploding or vanishing gradients. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embed) # embedding for each position\n",
    "\n",
    "        self.blocks = nn.Sequential(\n",
    "            TransformerBlock(n_embed, n_head=4),\n",
    "            TransformerBlock(n_embed, n_head=4),\n",
    "            TransformerBlock(n_embed, n_head=4)\n",
    "        )\n",
    "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        token_embeddings = self.token_embedding_table(idx) # Dimensions: Batch, Time, vocab size\n",
    "        positional_embedding = self.position_embedding_table(torch.arange(T, device=device)) # integers until T-1, Embeeded to create a T, C matrix\n",
    "        \n",
    "        x = token_embeddings + positional_embedding\n",
    "        x = self.blocks(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        if targets is None:\n",
    "            loss=None\n",
    "        else:\n",
    "            # Torch expects a different dimension for crossetropy\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skip/ residual connections\n",
    "\n",
    "Gradient Superhighway: During backpropagation, gradients are distributed equally at addition nodes. This means that a portion of the gradient from the loss function directly reaches earlier layers without being diminished by passing through intermediate layers. This creates a \"gradient superhighway\" that helps mitigate the vanishing gradient problem.<br>\n",
    "\n",
    "Easier Optimization: Residual blocks, which are the blocks with skip connections, are often initialized to have little impact on the network's output initially. This allows the network to start with a simpler structure that is easier to optimize. As training progresses, these blocks gradually contribute more, allowing the network to learn more complex features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, n_embed, n_head):\n",
    "        super.__init__()\n",
    "        head_size = n_embed // n_head\n",
    "        self.self_attention = MultiheadAttention(n_head, head_size)\n",
    "        self.ff = FeedForward(n_embed=n_embed)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.self_attention(x) # Skip connection \n",
    "        x = x + self.ff(x) # Skip connection \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this to work, the output of the heads and the feedforward net need to be projected back into the residual pathway. the paper also suggests to project to x4 inside the feedforward.|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadAttention(nn.Module):\n",
    "    def __init__(self,  num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([AttentionHead(head_size=head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embed, n_embed)\n",
    "    def forward(self, x):\n",
    "        multi_head_attention = torch.cat([h(x) for h in self.heads], dim=-1) # cat over channel dim\n",
    "        multi_head_attention = self.proj(x) # projection back into the residual pathway\n",
    "        return multi_head_attention\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embed) -> None:\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(nn.Linear(n_embed, n_embed*4),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Linear(n_embed*4, n_embed) # projection back into the residual pathway\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layernorm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How Layer Normalisation (LayerNorm) works\n",
    "- LayerNorm, normalises activations for each individual training example independently.\n",
    "- It calculates the mean and variance of activations across all neurons within a single layer for a given example.\n",
    "- Then, it uses these statistics to normalise the activations within that layer for that specific example.\n",
    "\n",
    "Gamma (γ) and beta (β) are learnable parameters in Layer Normalization that allow the network to undo the normalization if needed or to scale and shift the normalized values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key Benefits\n",
    "- Improved Optimisation: LayerNorm helps to smooth the optimisation landscape, making it easier for the optimiser to find a good solution.\n",
    "- Faster Convergence: By stabilising training, LayerNorm can lead to faster convergence to a good set of weights.\n",
    "- Better Generalisation: LayerNorm can improve the model's ability to generalise to unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 100])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LayerNorm1d: # (used to be BatchNorm1d)\n",
    "\n",
    "  def __init__(self, dim, eps=1e-5):\n",
    "    self.eps = eps\n",
    "    self.gamma = torch.ones(dim)\n",
    "    self.beta = torch.zeros(dim)\n",
    "\n",
    "  def __call__(self, x):\n",
    "    # calculate the forward pass\n",
    "    xmean = x.mean(1, keepdim=True) # batch mean, change 1 to 0 for norm across columns\n",
    "    xvar = x.var(1, keepdim=True) # batch variance, hange 1 to 0 for norm across columns\n",
    "    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n",
    "    self.out = self.gamma * xhat + self.beta\n",
    "    return self.out\n",
    "\n",
    "  def parameters(self):\n",
    "    return [self.gamma, self.beta]\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "module = LayerNorm1d(100)\n",
    "x = torch.randn(32, 100) # batch size 32 of 100-dimensional vectors\n",
    "x = module(x)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch implementation of layernorm in the transformer block. It used to be applied after the transformations such as self attention or feed forward but it has become common practice to do it before (prenorm formulation). the layernorm is also added again at the end before the llm head. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, n_embed, n_head):\n",
    "        super.__init__()\n",
    "        head_size = n_embed // n_head\n",
    "        self.self_attention = MultiheadAttention(n_head, head_size)\n",
    "        self.ff = FeedForward(n_embed=n_embed)\n",
    "        self.ln1 = nn.LayerNorm(n_embed)\n",
    "        self.ln2 = nn.LayerNorm(n_embed)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.self_attention(self.ln1(x)) # Skip connection , Layernorm applied before self attention\n",
    "        x = x + self.ff(self.ln2(x)) # Skip connection, Layernorm applied before feed forward\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To prevent overfitting, dropout should be added to the attentionheads, feedforward. This prevents some of the node from communicating by randomly dropping some weights. Every forward and random, some random of weights are being dropped which causes an ensable of network being trained. During test time, dropout is disabled again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Final notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The version of the transformer implemented in this notebook is a decoder only model because we are not using crossattention from an encoder and encoders also do not use triangular masking.\n",
    "    - In cross attention, the keys and values come from the endcoder. \n",
    "- The original paper that includes both encoder and decoder, focussed on machine translation, sequence to sequence tasks. \n",
    "- This notebook did not cover tokenizing, so far it only covered how to train a transformer on single letters"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
